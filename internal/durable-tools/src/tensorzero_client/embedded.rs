//! Embedded TensorZero client that uses gateway state directly.
//!
//! This implementation is used when the worker runs inside the gateway process
//! and wants to call inference and autopilot endpoints without HTTP overhead.

use std::collections::HashMap;
use std::sync::Arc;
use std::time::Duration;

use async_trait::async_trait;
use durable::WorkerOptions;
use evaluations::stats::EvaluationStats;
use evaluations::topk::{TopKTask, TopKTaskParams, TopKTaskState};
use evaluations::types::{EvaluationCoreArgs, EvaluationVariant};
use evaluations::{
    ClientInferenceExecutor, Clients, EvaluationUpdate, OutputFormat, run_evaluation_core_streaming,
};
use tensorzero::{
    ActionResponse, ClientBuilder, ClientBuilderMode, ClientInferenceParams,
    CreateDatapointRequest, CreateDatapointsFromInferenceRequestParams, CreateDatapointsResponse,
    DeleteDatapointsResponse, FeedbackParams, FeedbackResponse, GetConfigResponse,
    GetDatapointsResponse, GetInferencesResponse, InferenceOutput, InferenceResponse,
    ListDatapointsRequest, ListInferencesRequest, TensorZeroError, UpdateDatapointRequest,
    UpdateDatapointsResponse, WriteConfigRequest, WriteConfigResponse,
};
use tensorzero_core::config::snapshot::{ConfigSnapshot, SnapshotHash};
use tensorzero_core::config::write_config_snapshot;
use tensorzero_core::db::ConfigQueries;
use tensorzero_core::db::feedback::FeedbackByVariant;
use tensorzero_core::db::feedback::FeedbackQueries;
use tensorzero_core::endpoints::datasets::v1::types::{
    CreateDatapointsFromInferenceRequest, CreateDatapointsRequest, DeleteDatapointsRequest,
    GetDatapointsRequest, UpdateDatapointsRequest,
};
use tensorzero_core::endpoints::feedback::feedback;
use tensorzero_core::endpoints::feedback::internal::LatestFeedbackIdByMetricResponse;
use tensorzero_core::endpoints::inference::inference;
use tensorzero_core::endpoints::internal::action::{ActionInput, ActionInputInfo, action};
use tensorzero_core::endpoints::internal::autopilot::{create_event, list_events, list_sessions};
use tensorzero_core::error::{Error, ErrorDetails};
use tensorzero_core::evaluations::{EvaluationConfig, EvaluationFunctionConfig};
use tensorzero_core::utils::gateway::AppStateData;
use uuid::Uuid;

use super::{
    CreateEventGatewayRequest, CreateEventResponse, EvaluatorStatsResponse, ListEventsParams,
    ListEventsResponse, ListSessionsParams, ListSessionsResponse, RunEvaluationParams,
    RunEvaluationResponse, RunTopKEvaluationParams, RunTopKEvaluationResponse, TensorZeroClient,
    TensorZeroClientError,
};

/// TensorZero client that uses an existing gateway's state directly.
///
/// This is used when the worker runs inside the gateway process and wants to
/// call inference and autopilot endpoints without HTTP overhead.
pub struct EmbeddedClient {
    app_state: AppStateData,
}

impl EmbeddedClient {
    /// Create a new embedded client from gateway state.
    pub fn new(app_state: AppStateData) -> Self {
        Self { app_state }
    }
}

#[async_trait]
impl TensorZeroClient for EmbeddedClient {
    async fn inference(
        &self,
        params: ClientInferenceParams,
    ) -> Result<InferenceResponse, TensorZeroClientError> {
        let internal_params = params
            .try_into()
            .map_err(|e: tensorzero_core::error::Error| {
                TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
            })?;

        let result = Box::pin(inference(
            self.app_state.config.clone(),
            &self.app_state.http_client,
            self.app_state.clickhouse_connection_info.clone(),
            self.app_state.postgres_connection_info.clone(),
            self.app_state.deferred_tasks.clone(),
            internal_params,
            None, // No API key in embedded mode
        ))
        .await
        .map_err(|e| {
            TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
        })?;

        match result.output {
            InferenceOutput::NonStreaming(response) => Ok(response),
            InferenceOutput::Streaming(_) => Err(TensorZeroClientError::StreamingNotSupported),
        }
    }

    async fn feedback(
        &self,
        params: FeedbackParams,
    ) -> Result<FeedbackResponse, TensorZeroClientError> {
        feedback(self.app_state.clone(), params, None)
            .await
            .map_err(|e| {
                TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
            })
    }

    async fn create_autopilot_event(
        &self,
        session_id: Uuid,
        request: CreateEventGatewayRequest,
    ) -> Result<CreateEventResponse, TensorZeroClientError> {
        let autopilot_client = self
            .app_state
            .autopilot_client
            .as_ref()
            .ok_or(TensorZeroClientError::AutopilotUnavailable)?;

        // Get deployment_id from app_state
        let deployment_id = self
            .app_state
            .deployment_id
            .clone()
            .ok_or(TensorZeroClientError::AutopilotUnavailable)?;

        // Construct the full request with deployment_id from app state
        let full_request = autopilot_client::CreateEventRequest {
            deployment_id,
            tensorzero_version: tensorzero_core::endpoints::status::TENSORZERO_VERSION.to_string(),
            payload: request.payload,
            previous_user_message_event_id: request.previous_user_message_event_id,
        };

        create_event(autopilot_client, session_id, full_request)
            .await
            .map_err(|e| {
                TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
            })
    }

    async fn list_autopilot_events(
        &self,
        session_id: Uuid,
        params: ListEventsParams,
    ) -> Result<ListEventsResponse, TensorZeroClientError> {
        let autopilot_client = self
            .app_state
            .autopilot_client
            .as_ref()
            .ok_or(TensorZeroClientError::AutopilotUnavailable)?;

        list_events(autopilot_client, session_id, params)
            .await
            .map_err(|e| {
                TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
            })
    }

    async fn list_autopilot_sessions(
        &self,
        params: ListSessionsParams,
    ) -> Result<ListSessionsResponse, TensorZeroClientError> {
        let autopilot_client = self
            .app_state
            .autopilot_client
            .as_ref()
            .ok_or(TensorZeroClientError::AutopilotUnavailable)?;

        list_sessions(autopilot_client, params).await.map_err(|e| {
            TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
        })
    }

    async fn action(
        &self,
        snapshot_hash: SnapshotHash,
        input: ActionInput,
    ) -> Result<InferenceResponse, TensorZeroClientError> {
        let action_input = ActionInputInfo {
            snapshot_hash,
            input,
        };

        let response = action(&self.app_state, action_input).await.map_err(|e| {
            TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
        })?;

        match response {
            ActionResponse::Inference(r) => Ok(r),
            ActionResponse::Feedback(_) => {
                Err(TensorZeroClientError::TensorZero(TensorZeroError::Other {
                    source: tensorzero_core::error::Error::new(
                        tensorzero_core::error::ErrorDetails::InternalError {
                            message: "Unexpected feedback response from action endpoint"
                                .to_string(),
                        },
                    )
                    .into(),
                }))
            }
        }
    }

    async fn get_config_snapshot(
        &self,
        hash: Option<String>,
    ) -> Result<GetConfigResponse, TensorZeroClientError> {
        let snapshot_hash = match hash {
            Some(hash) => hash.parse().map_err(|_| {
                TensorZeroClientError::TensorZero(TensorZeroError::Other {
                    source: Error::new(ErrorDetails::ConfigSnapshotNotFound {
                        snapshot_hash: hash,
                    })
                    .into(),
                })
            })?,
            None => self.app_state.config.hash.clone(),
        };

        let snapshot = self
            .app_state
            .clickhouse_connection_info
            .get_config_snapshot(snapshot_hash)
            .await
            .map_err(|e| {
                TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
            })?;

        Ok(GetConfigResponse {
            hash: snapshot.hash.to_string(),
            config: snapshot.config.into(),
            extra_templates: snapshot.extra_templates,
            tags: snapshot.tags,
        })
    }

    async fn write_config(
        &self,
        request: WriteConfigRequest,
    ) -> Result<WriteConfigResponse, TensorZeroClientError> {
        let mut snapshot =
            ConfigSnapshot::new(request.config, request.extra_templates).map_err(|e| {
                TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
            })?;
        snapshot.tags = request.tags;

        let hash = snapshot.hash.to_string();

        write_config_snapshot(&self.app_state.clickhouse_connection_info, snapshot)
            .await
            .map_err(|e| {
                TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
            })?;

        Ok(WriteConfigResponse { hash })
    }

    // ========== Datapoint CRUD Operations ==========

    async fn create_datapoints(
        &self,
        dataset_name: String,
        datapoints: Vec<CreateDatapointRequest>,
    ) -> Result<CreateDatapointsResponse, TensorZeroClientError> {
        let request = CreateDatapointsRequest { datapoints };

        tensorzero_core::endpoints::datasets::v1::create_datapoints(
            &self.app_state.config,
            &self.app_state.http_client,
            &self.app_state.clickhouse_connection_info,
            &dataset_name,
            request,
        )
        .await
        .map_err(|e| TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() }))
    }

    async fn create_datapoints_from_inferences(
        &self,
        dataset_name: String,
        params: CreateDatapointsFromInferenceRequestParams,
    ) -> Result<CreateDatapointsResponse, TensorZeroClientError> {
        let request = CreateDatapointsFromInferenceRequest { params };

        tensorzero_core::endpoints::datasets::v1::create_from_inferences(
            &self.app_state.config,
            &self.app_state.clickhouse_connection_info,
            dataset_name,
            request,
        )
        .await
        .map_err(|e| TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() }))
    }

    async fn list_datapoints(
        &self,
        dataset_name: String,
        request: ListDatapointsRequest,
    ) -> Result<GetDatapointsResponse, TensorZeroClientError> {
        tensorzero_core::endpoints::datasets::v1::list_datapoints(
            &self.app_state.clickhouse_connection_info,
            dataset_name,
            request,
        )
        .await
        .map_err(|e| TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() }))
    }

    async fn get_datapoints(
        &self,
        dataset_name: Option<String>,
        ids: Vec<Uuid>,
    ) -> Result<GetDatapointsResponse, TensorZeroClientError> {
        let request = GetDatapointsRequest { ids };

        tensorzero_core::endpoints::datasets::v1::get_datapoints(
            &self.app_state.clickhouse_connection_info,
            dataset_name,
            request,
        )
        .await
        .map_err(|e| TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() }))
    }

    async fn update_datapoints(
        &self,
        dataset_name: String,
        datapoints: Vec<UpdateDatapointRequest>,
    ) -> Result<UpdateDatapointsResponse, TensorZeroClientError> {
        let request = UpdateDatapointsRequest { datapoints };

        tensorzero_core::endpoints::datasets::v1::update_datapoints(
            &self.app_state,
            &dataset_name,
            request,
        )
        .await
        .map_err(|e| TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() }))
    }

    async fn delete_datapoints(
        &self,
        dataset_name: String,
        ids: Vec<Uuid>,
    ) -> Result<DeleteDatapointsResponse, TensorZeroClientError> {
        let request = DeleteDatapointsRequest { ids };

        tensorzero_core::endpoints::datasets::v1::delete_datapoints(
            &self.app_state.clickhouse_connection_info,
            &dataset_name,
            request,
        )
        .await
        .map_err(|e| TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() }))
    }

    // ========== Inference Query Operations ==========

    async fn list_inferences(
        &self,
        request: ListInferencesRequest,
    ) -> Result<GetInferencesResponse, TensorZeroClientError> {
        tensorzero_core::endpoints::stored_inferences::v1::list_inferences(
            &self.app_state.config,
            &self.app_state.clickhouse_connection_info,
            request,
        )
        .await
        .map_err(|e| TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() }))
    }

    // ========== Optimization Operations ==========

    async fn launch_optimization_workflow(
        &self,
        params: tensorzero_optimizers::endpoints::LaunchOptimizationWorkflowParams,
    ) -> Result<super::OptimizationJobHandle, TensorZeroClientError> {
        tensorzero_optimizers::endpoints::launch_optimization_workflow(
            &self.app_state.http_client,
            self.app_state.config.clone(),
            &self.app_state.clickhouse_connection_info,
            params,
        )
        .await
        .map_err(|e| TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() }))
    }

    async fn poll_optimization(
        &self,
        job_handle: &super::OptimizationJobHandle,
    ) -> Result<super::OptimizationJobInfo, TensorZeroClientError> {
        tensorzero_optimizers::endpoints::poll_optimization(
            &self.app_state.http_client,
            job_handle,
            &self.app_state.config.models.default_credentials,
            &self.app_state.config.provider_types,
        )
        .await
        .map_err(|e| TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() }))
    }

    async fn get_latest_feedback_id_by_metric(
        &self,
        target_id: Uuid,
    ) -> Result<LatestFeedbackIdByMetricResponse, TensorZeroClientError> {
        tensorzero_core::endpoints::feedback::internal::get_latest_feedback_id_by_metric(
            &self.app_state.clickhouse_connection_info,
            target_id,
        )
        .await
        .map_err(|e| TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() }))
    }

    async fn get_feedback_by_variant(
        &self,
        metric_name: String,
        function_name: String,
        variant_names: Option<Vec<String>>,
    ) -> Result<Vec<FeedbackByVariant>, TensorZeroClientError> {
        self.app_state
            .clickhouse_connection_info
            .get_feedback_by_variant(&metric_name, &function_name, variant_names.as_ref())
            .await
            .map_err(|e| {
                TensorZeroClientError::TensorZero(TensorZeroError::Other { source: e.into() })
            })
    }

    async fn run_evaluation(
        &self,
        params: RunEvaluationParams,
    ) -> Result<RunEvaluationResponse, TensorZeroClientError> {
        // Look up the evaluation config
        let evaluation_config = self
            .app_state
            .config
            .evaluations
            .get(&params.evaluation_name)
            .ok_or_else(|| {
                TensorZeroClientError::Evaluation(format!(
                    "Evaluation '{}' not found in config",
                    params.evaluation_name
                ))
            })?
            .clone();

        // Build function configs table for the evaluation
        let function_configs: HashMap<String, EvaluationFunctionConfig> = self
            .app_state
            .config
            .functions
            .iter()
            .map(|(name, func)| (name.clone(), EvaluationFunctionConfig::from(func.as_ref())))
            .collect();
        let function_configs = Arc::new(function_configs);

        // Build a Client from our existing components
        let tensorzero_client = ClientBuilder::new(ClientBuilderMode::FromComponents {
            config: self.app_state.config.clone(),
            clickhouse_connection_info: self.app_state.clickhouse_connection_info.clone(),
            postgres_connection_info: self.app_state.postgres_connection_info.clone(),
            http_client: self.app_state.http_client.clone(),
            timeout: None,
        })
        .build()
        .await
        .map_err(|e| TensorZeroClientError::Evaluation(format!("Failed to build client: {e}")))?;

        let evaluation_run_id = Uuid::now_v7();

        // Wrap the client in ClientInferenceExecutor for use with evaluations
        let inference_executor = Arc::new(ClientInferenceExecutor::new(tensorzero_client));

        let core_args = EvaluationCoreArgs {
            inference_executor,
            clickhouse_client: self.app_state.clickhouse_connection_info.clone(),
            evaluation_config,
            function_configs,
            dataset_name: params.dataset_name,
            datapoint_ids: params.datapoint_ids,
            variant: EvaluationVariant::Name(params.variant_name),
            evaluation_name: params.evaluation_name,
            evaluation_run_id,
            inference_cache: params.inference_cache,
            concurrency: params.concurrency,
        };

        // Run the evaluation with optional adaptive stopping via precision_targets
        let result = run_evaluation_core_streaming(
            core_args,
            params.max_datapoints,
            params.precision_targets,
        )
        .await
        .map_err(|e| TensorZeroClientError::Evaluation(format!("Evaluation failed: {e}")))?;

        let mut receiver = result.receiver;
        let num_datapoints = result.run_info.num_datapoints;

        // Collect results - we use a dummy writer since we don't need CLI output
        let mut evaluation_stats = EvaluationStats::new(OutputFormat::Jsonl, num_datapoints);
        let mut dummy_writer = std::io::sink();

        while let Some(update) = receiver.recv().await {
            match update {
                EvaluationUpdate::RunInfo(_) => {
                    // Skip RunInfo
                    continue;
                }
                update => {
                    // Ignore write errors to the dummy sink
                    let _ = evaluation_stats.push(update, &mut dummy_writer);
                }
            }
        }

        // Compute statistics
        let EvaluationConfig::Inference(inference_config) = &*result.evaluation_config;
        let stats = evaluation_stats.compute_stats(&inference_config.evaluators);

        // Convert to response format
        let stats_response: HashMap<String, EvaluatorStatsResponse> = stats
            .into_iter()
            .map(|(name, s)| {
                (
                    name,
                    EvaluatorStatsResponse {
                        mean: s.mean,
                        stderr: s.stderr,
                        count: s.count,
                    },
                )
            })
            .collect();

        Ok(RunEvaluationResponse {
            evaluation_run_id,
            num_datapoints,
            num_successes: evaluation_stats.evaluation_infos.len(),
            num_errors: evaluation_stats.evaluation_errors.len(),
            stats: stats_response,
        })
    }

    async fn run_topk_evaluation(
        &self,
        params: RunTopKEvaluationParams,
    ) -> Result<RunTopKEvaluationResponse, TensorZeroClientError> {
        // Look up the evaluation config
        let evaluation_config = self
            .app_state
            .config
            .evaluations
            .get(&params.evaluation_name)
            .ok_or_else(|| {
                TensorZeroClientError::Evaluation(format!(
                    "Evaluation '{}' not found in config",
                    params.evaluation_name
                ))
            })?
            .clone();

        // Build function configs table
        let function_configs: HashMap<String, EvaluationFunctionConfig> = self
            .app_state
            .config
            .functions
            .iter()
            .map(|(name, func)| (name.clone(), EvaluationFunctionConfig::from(func.as_ref())))
            .collect();

        // Build TopKTaskParams from RunTopKEvaluationParams
        let task_params = TopKTaskParams {
            evaluation_name: params.evaluation_name,
            dataset_name: params.dataset_name,
            variant_names: params.variant_names,
            k_min: params.k_min,
            k_max: params.k_max,
            epsilon: params.epsilon,
            max_datapoints: params.max_datapoints,
            batch_size: params.batch_size,
            variant_failure_threshold: params.variant_failure_threshold,
            evaluator_failure_threshold: params.evaluator_failure_threshold,
            concurrency: params.concurrency,
            inference_cache: params.inference_cache,
            evaluation_config: (*evaluation_config).clone(),
            function_configs,
            scoring_function: params.scoring_function,
        };

        // Build a Client from our existing components for inference
        let tensorzero_client = ClientBuilder::new(ClientBuilderMode::FromComponents {
            config: self.app_state.config.clone(),
            clickhouse_connection_info: self.app_state.clickhouse_connection_info.clone(),
            postgres_connection_info: self.app_state.postgres_connection_info.clone(),
            http_client: self.app_state.http_client.clone(),
            timeout: None,
        })
        .build()
        .await
        .map_err(|e| TensorZeroClientError::Evaluation(format!("Failed to build client: {e}")))?;

        // Create task state with clients
        let inference_executor = Arc::new(ClientInferenceExecutor::new(tensorzero_client));
        let clients = Arc::new(Clients {
            inference_executor,
            clickhouse_client: self.app_state.clickhouse_connection_info.clone(),
        });
        let task_state = TopKTaskState { clients };

        // Get postgres pool from gateway
        let pg_pool = self
            .app_state
            .postgres_connection_info
            .get_pool()
            .ok_or_else(|| {
                TensorZeroClientError::Evaluation(
                    "PostgreSQL connection required for top-k evaluation".to_string(),
                )
            })?;

        // Create durable client with unique queue name
        let queue_name = format!("topk_eval_{}", Uuid::now_v7());
        let durable_client = evaluations::topk::create_client(
            pg_pool.clone(),
            task_state.clone(),
            Some(&queue_name),
        )
        .await
        .map_err(|e| {
            TensorZeroClientError::Evaluation(format!("Failed to create durable client: {e}"))
        })?;

        // Spawn the task
        let spawn_result = durable_client
            .spawn::<TopKTask>(task_params)
            .await
            .map_err(|e| TensorZeroClientError::Evaluation(format!("Failed to spawn task: {e}")))?;

        // Start a worker to process the task
        let worker = durable_client
            .start_worker(WorkerOptions {
                poll_interval: Duration::from_millis(100),
                claim_timeout: Duration::from_secs(300),
                ..Default::default()
            })
            .await
            .map_err(|e| {
                TensorZeroClientError::Evaluation(format!("Failed to start worker: {e}"))
            })?;

        // Poll for completion
        let output = poll_topk_task(pg_pool, &queue_name, spawn_result.task_id).await;

        // Shutdown worker
        worker.shutdown().await;

        // Return result or error
        let output = output?;
        Ok(RunTopKEvaluationResponse { output })
    }
}

/// Poll for top-k task completion.
async fn poll_topk_task(
    pool: &sqlx::PgPool,
    queue_name: &str,
    task_id: Uuid,
) -> Result<evaluations::topk::TopKTaskOutput, TensorZeroClientError> {
    use sqlx::{AssertSqlSafe, query_as};

    let timeout = Duration::from_secs(3600); // 1 hour timeout
    let start = std::time::Instant::now();

    loop {
        if start.elapsed() > timeout {
            return Err(TensorZeroClientError::Evaluation(
                "Top-k evaluation timed out".to_string(),
            ));
        }

        // Check task state
        let query = format!("SELECT state FROM durable.t_{queue_name} WHERE task_id = $1");
        let state: Option<(String,)> = query_as(AssertSqlSafe(query))
            .bind(task_id)
            .fetch_optional(pool)
            .await
            .map_err(|e| {
                TensorZeroClientError::Evaluation(format!("Failed to query task state: {e}"))
            })?;

        if let Some((state,)) = state {
            if state == "completed" {
                break;
            } else if state == "failed" {
                // Get error message
                let query =
                    format!("SELECT failed_error FROM durable.t_{queue_name} WHERE task_id = $1");
                let error: Option<(Option<String>,)> = query_as(AssertSqlSafe(query))
                    .bind(task_id)
                    .fetch_optional(pool)
                    .await
                    .ok()
                    .flatten();
                let error_msg = error
                    .and_then(|(e,)| e)
                    .unwrap_or_else(|| "Unknown error".to_string());
                return Err(TensorZeroClientError::Evaluation(format!(
                    "Top-k task failed: {error_msg}"
                )));
            }
        }

        tokio::time::sleep(Duration::from_millis(500)).await;
    }

    // Get the task result
    let query = format!("SELECT completed_payload FROM durable.t_{queue_name} WHERE task_id = $1");
    let result: Option<(Option<serde_json::Value>,)> = query_as(AssertSqlSafe(query))
        .bind(task_id)
        .fetch_optional(pool)
        .await
        .map_err(|e| {
            TensorZeroClientError::Evaluation(format!("Failed to query task result: {e}"))
        })?;

    let output = result
        .and_then(|(payload,)| payload)
        .ok_or_else(|| TensorZeroClientError::Evaluation("No task output found".to_string()))?;

    serde_json::from_value(output).map_err(|e| {
        TensorZeroClientError::Evaluation(format!("Failed to deserialize output: {e}"))
    })
}
