# KIE Reasoning Example - Setup Summary

## âœ… KIE Example Setup Complete

A complete KIE (Knowledge-Intensive Engine) provider example has been created in the `examples/kie-reasoning` directory.

### ğŸ“ Directory Structure

```
examples/kie-reasoning/
â”œâ”€â”€ README.md                      # Comprehensive documentation (English)
â”œâ”€â”€ QUICKSTART.md                  # Quick start guide (English)
â”œâ”€â”€ example.py                     # Python example code
â”œâ”€â”€ example.ts                     # TypeScript/Node.js example code
â”œâ”€â”€ package.json                   # Node.js dependencies
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ tsconfig.json                  # TypeScript configuration
â””â”€â”€ config/
    â”œâ”€â”€ tensorzero.toml           # KIE configuration
    â”œâ”€â”€ math_system_prompt.txt    # Math reasoning system prompt
    â””â”€â”€ code_system_prompt.txt    # Code analysis system prompt
```

### ğŸ¯ Included Examples

1. **Problem Solving** - Demonstrates streaming responses with medium reasoning effort
2. **Math Reasoning** - Uses high reasoning effort to prove mathematical theorems
3. **Code Analysis** - Real-time streaming feedback for code review
4. **Model Comparison** - Compare KIE with GPT-4 Turbo on same prompts

### ğŸš€ Quick Start

**Python:**
```bash
export KIE_API_KEY="your-api-key"
pip install -r requirements.txt
python example.py
```

**TypeScript:**
```bash
export KIE_API_KEY="your-api-key"
npm install
npm start
```

### ğŸ“‹ Configuration Features

- âœ… Multiple KIE variants (different reasoning effort levels)
- âœ… Streaming and non-streaming inference
- âœ… Thought blocks/reasoning content support
- âœ… Tool calling infrastructure
- âœ… Custom system prompts
- âœ… Model comparison setup

### ğŸ”‘ Key Configuration Parameters

```toml
model = "kie::gemini-3-pro"           # KIE model identifier
max_tokens = 16000                # Maximum token limit
reasoning_effort = "medium"       # "low", "medium", or "high"
include_thoughts = true           # Include reasoning steps
stream = true                     # Enable streaming responses
```

### ğŸ“– Documentation

- **README.md** - Complete English documentation with all features and parameters
- **QUICKSTART.md** - Quick start guide with troubleshooting
- Detailed code comments in Python and TypeScript examples

### ğŸ¯ Supported Use Cases

1. **Academic Research** - Use `reasoning_effort = "high"` for deep analysis
2. **Real-time Chat** - Use `reasoning_effort = "low"` with streaming
3. **Code Review** - Medium reasoning with custom system prompts
4. **A/B Testing** - Compare different reasoning effort levels

### âœ… Following DeepSeek Best Practices

This example follows TensorZero's DeepSeek provider pattern:
- Uses consistent configuration structure
- Demonstrates both streaming and non-streaming inference
- Shows how to integrate multiple variants
- Includes system prompt templates
- Provides comprehensive documentation and examples
