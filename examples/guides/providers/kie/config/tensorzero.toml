# TensorZero Configuration for KIE (Knowledge-Intensive Engine) Reasoning

[functions.solve_problem]
type = "chat"
description = "Solve a complex problem using KIE reasoning"

# KIE Gemini 3 Pro with reasoning enabled
[functions.solve_problem.variants.kie_reasoning]
type = "chat_completion"
model = "kie::gemini-3-pro"
max_tokens = 16000
reasoning_effort = "high"

# KIE Gemini 3 Pro with extended reasoning
[functions.solve_problem.variants.kie_extended]
type = "chat_completion"
model = "kie::gemini-3-pro"
max_tokens = 32000
reasoning_effort = "high"

# KIE Gemini 3 Flash (faster inference)
[functions.solve_problem.variants.kie_flash]
type = "chat_completion"
model = "kie::gemini-3-flash"
max_tokens = 4000
temperature = 0.7

# KIE Gemini 2.5 Pro with reasoning
[functions.solve_problem.variants.kie_gemini_2_5_pro]
type = "chat_completion"
model = "kie::gemini-2.5-pro"
max_tokens = 16000
reasoning_effort = "high"

# KIE Gemini 2.5 Flash (fastest variant)
[functions.solve_problem.variants.kie_gemini_2_5_flash]
type = "chat_completion"
model = "kie::gemini-2.5-flash"
max_tokens = 8000
temperature = 0.7

# Comparison: OpenAI for baseline
[functions.solve_problem.variants.gpt_4_turbo]
type = "chat_completion"
model = "openai::gpt-4-turbo"
max_tokens = 16000
temperature = 0.7

[functions.math_reasoning]
type = "chat"
description = "Solve math problems with step-by-step reasoning"

# KIE Gemini 3 Pro for math reasoning with maximum reasoning effort
[functions.math_reasoning.variants.kie_math]
type = "chat_completion"
model = "kie::gemini-3-pro"
max_tokens = 8000
reasoning_effort = "high"
system_template = "math_system_prompt.txt"

# KIE Gemini 2.5 Pro alternative
[functions.math_reasoning.variants.kie_math_2_5]
type = "chat_completion"
model = "kie::gemini-2.5-pro"
max_tokens = 8000
reasoning_effort = "high"
system_template = "math_system_prompt.txt"

[functions.code_analysis]
type = "chat"
description = "Analyze code and provide detailed explanations"

# KIE Gemini 3 Pro for code analysis
[functions.code_analysis.variants.kie_code]
type = "chat_completion"
model = "kie::gemini-3-pro"
max_tokens = 12000
reasoning_effort = "high"
system_template = "code_system_prompt.txt"

# KIE Gemini 3 Flash for faster code analysis
[functions.code_analysis.variants.kie_code_flash]
type = "chat_completion"
model = "kie::gemini-3-flash"
max_tokens = 12000
reasoning_effort = "low"
system_template = "code_system_prompt.txt"

# Streaming variant for real-time feedback (streaming is controlled via API request)
[functions.code_analysis.variants.kie_code_streaming]
type = "chat_completion"
model = "kie::gemini-3-pro"
max_tokens = 12000
reasoning_effort = "low"
system_template = "code_system_prompt.txt"
# ========================================================================
# Media Processing Functions
# ========================================================================

[functions.image_analysis]
type = "chat"
description = "Analyze images and provide detailed descriptions"

# KIE Gemini 3 Pro for image analysis
[functions.image_analysis.variants.kie_vision]
type = "chat_completion"
model = "kie::gemini-3-pro"
reasoning_effort = "high"

# KIE Gemini 3 Flash for faster image analysis
[functions.image_analysis.variants.kie_vision_fast]
type = "chat_completion"
model = "kie::gemini-3-flash"
max_tokens = 2000
reasoning_effort = "low"

[functions.video_analysis]
type = "chat"
description = "Analyze videos and extract key information"

# KIE Gemini 3 Pro for video analysis
[functions.video_analysis.variants.kie_video]
type = "chat_completion"
model = "kie::gemini-3-pro"
max_tokens = 6000
reasoning_effort = "high"

# KIE Gemini 2.5 Pro alternative for video
[functions.video_analysis.variants.kie_video_2_5]
type = "chat_completion"
model = "kie::gemini-2.5-pro"
max_tokens = 6000
reasoning_effort = "high"

[functions.document_analysis]
type = "chat"
description = "Analyze PDF documents and extract insights"

# KIE Gemini 3 Pro for document analysis
[functions.document_analysis.variants.kie_document]
type = "chat_completion"
model = "kie::gemini-3-pro"
max_tokens = 8000
reasoning_effort = "high"

# KIE Gemini 3 Flash for faster document analysis
[functions.document_analysis.variants.kie_document_fast]
type = "chat_completion"
model = "kie::gemini-3-flash"
max_tokens = 4000
reasoning_effort = "low"

[functions.multimodal_reasoning]
type = "chat"
description = "Perform reasoning across multiple media types (images, videos, documents)"

# KIE Gemini 3 Pro for advanced multimodal reasoning
[functions.multimodal_reasoning.variants.kie_multimodal]
type = "chat_completion"
model = "kie::gemini-3-pro"
max_tokens = 10000
reasoning_effort = "high"

# KIE Gemini 2.5 Pro for multimodal processing
[functions.multimodal_reasoning.variants.kie_multimodal_2_5]
type = "chat_completion"
model = "kie::gemini-2.5-pro"
max_tokens = 10000
reasoning_effort = "high"

# KIE Gemini 3 Flash for faster multimodal analysis
[functions.multimodal_reasoning.variants.kie_multimodal_fast]
type = "chat_completion"
model = "kie::gemini-3-flash"
max_tokens = 6000
reasoning_effort = "low"

[functions.extract_data]
type = "chat"

# KIE Gemini 3 Pro for advanced multimodal reasoning
[functions.extract_data.variants.kie_multimodal]
type = "chat_completion"
model = "kie::gemini-3-pro"
max_tokens = 10000
reasoning_effort = "high"

# KIE Gemini 2.5 Pro for multimodal processing
[functions.extract_data.variants.kie_multimodal_2_5]
type = "chat_completion"
model = "kie::gemini-2.5-pro"
max_tokens = 10000
reasoning_effort = "high"

# KIE Gemini 3 Flash for faster multimodal analysis
[functions.extract_data.variants.kie_multimodal_fast]
type = "chat_completion"
model = "kie::gemini-3-flash"
max_tokens = 6000
reasoning_effort = "low"

[functions.extract_data.experimentation]
type = "uniform"
candidate_variants = ["kie_multimodal_2_5", "kie_multimodal_fast"]
fallback_variants = ["kie_multimodal"] 

[gateway]
fetch_and_encode_input_files_before_inference = true

[object_storage]
type = "s3_compatible"
endpoint = "http://minio:9000"  # optional: defaults to AWS S3
# region = "us-east-1"  # optional: depends on your S3-compatible storage provider
bucket_name = "tensorzero"  # optional: depends on your S3-compatible storage provider
# IMPORTANT: for production environments, remove the following setting and use a secure method of authentication in
# combination with a production-grade object storage service.
allow_http = true